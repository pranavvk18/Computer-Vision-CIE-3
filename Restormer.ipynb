{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tPSJeEirxhK_"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== Restormer Components ====================\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"Layer Normalization that supports two data formats: channels_last and channels_first.\"\"\"\n",
        "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.eps = eps\n",
        "        self.data_format = data_format\n",
        "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
        "            raise NotImplementedError\n",
        "        self.normalized_shape = (normalized_shape, )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.data_format == \"channels_last\":\n",
        "            return nn.functional.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "        elif self.data_format == \"channels_first\":\n",
        "            u = x.mean(1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.eps)\n",
        "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
        "            return x\n",
        "\n",
        "\n",
        "class GDFN(nn.Module):\n",
        "    \"\"\"Gated-Dconv Feed-Forward Network\"\"\"\n",
        "    def __init__(self, dim, ffn_expansion_factor=2.66, bias=False):\n",
        "        super().__init__()\n",
        "        hidden_features = int(dim * ffn_expansion_factor)\n",
        "\n",
        "        self.project_in = nn.Conv2d(dim, hidden_features * 2, kernel_size=1, bias=bias)\n",
        "        self.dwconv = nn.Conv2d(hidden_features * 2, hidden_features * 2, kernel_size=3,\n",
        "                               stride=1, padding=1, groups=hidden_features * 2, bias=bias)\n",
        "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.project_in(x)\n",
        "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
        "        x = nn.functional.gelu(x1) * x2\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MDTA(nn.Module):\n",
        "    \"\"\"Multi-Dconv Head Transposed Attention\"\"\"\n",
        "    def __init__(self, dim, num_heads=8, bias=False):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
        "\n",
        "        self.qkv = nn.Conv2d(dim, dim * 3, kernel_size=1, bias=bias)\n",
        "        self.qkv_dwconv = nn.Conv2d(dim * 3, dim * 3, kernel_size=3, stride=1,\n",
        "                                    padding=1, groups=dim * 3, bias=bias)\n",
        "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "\n",
        "        qkv = self.qkv_dwconv(self.qkv(x))\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "\n",
        "        q = q.reshape(b, self.num_heads, -1, h * w)\n",
        "        k = k.reshape(b, self.num_heads, -1, h * w)\n",
        "        v = v.reshape(b, self.num_heads, -1, h * w)\n",
        "\n",
        "        q = torch.nn.functional.normalize(q, dim=-1)\n",
        "        k = torch.nn.functional.normalize(k, dim=-1)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        out = (attn @ v)\n",
        "        out = out.reshape(b, -1, h, w)\n",
        "\n",
        "        out = self.project_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Restormer Transformer Block\"\"\"\n",
        "    def __init__(self, dim, num_heads=8, ffn_expansion_factor=2.66, bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm1 = LayerNorm(dim, data_format='channels_first')\n",
        "        self.attn = MDTA(dim, num_heads, bias)\n",
        "        self.norm2 = LayerNorm(dim, data_format='channels_first')\n",
        "        self.ffn = GDFN(dim, ffn_expansion_factor, bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.ffn(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class OverlapPatchEmbed(nn.Module):\n",
        "    \"\"\"Overlapping Patch Embedding\"\"\"\n",
        "    def __init__(self, in_c=3, embed_dim=48, bias=False):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=3, stride=1, padding=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    \"\"\"Downsampling layer\"\"\"\n",
        "    def __init__(self, n_feat):\n",
        "        super().__init__()\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Conv2d(n_feat, n_feat // 2, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.PixelUnshuffle(2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.body(x)\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    \"\"\"Upsampling layer\"\"\"\n",
        "    def __init__(self, n_feat):\n",
        "        super().__init__()\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Conv2d(n_feat, n_feat * 2, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.PixelShuffle(2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.body(x)\n",
        "\n",
        "\n",
        "class Restormer(nn.Module):\n",
        "    \"\"\"\n",
        "    Restormer: Efficient Transformer for High-Resolution Image Restoration (CVPR 2022)\n",
        "    Paper: https://arxiv.org/abs/2111.09881\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 inp_channels=3,\n",
        "                 out_channels=3,\n",
        "                 dim=48,\n",
        "                 num_blocks=[4, 6, 6, 8],\n",
        "                 num_refinement_blocks=4,\n",
        "                 heads=[1, 2, 4, 8],\n",
        "                 ffn_expansion_factor=2.66,\n",
        "                 bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = OverlapPatchEmbed(inp_channels, dim)\n",
        "\n",
        "        self.encoder_level1 = nn.Sequential(\n",
        "            *[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor,\n",
        "                             bias=bias) for i in range(num_blocks[0])])\n",
        "\n",
        "        self.down1_2 = Downsample(dim)\n",
        "        self.encoder_level2 = nn.Sequential(\n",
        "            *[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
        "                             bias=bias) for i in range(num_blocks[1])])\n",
        "\n",
        "        self.down2_3 = Downsample(int(dim*2**1))\n",
        "        self.encoder_level3 = nn.Sequential(\n",
        "            *[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor,\n",
        "                             bias=bias) for i in range(num_blocks[2])])\n",
        "\n",
        "        self.down3_4 = Downsample(int(dim*2**2))\n",
        "        self.latent = nn.Sequential(\n",
        "            *[TransformerBlock(dim=int(dim*2**3), num_heads=heads[3], ffn_expansion_factor=ffn_expansion_factor,\n",
        "                             bias=bias) for i in range(num_blocks[3])])\n",
        "\n",
        "        self.up4_3 = Upsample(int(dim*2**3))\n",
        "        self.reduce_chan_level3 = nn.Conv2d(int(dim*2**3), int(dim*2**2), kernel_size=1, bias=bias)\n",
        "        self.decoder_level3 = nn.Sequential(\n",
        "            *[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor,\n",
        "                             bias=bias) for i in range(num_blocks[2])])\n",
        "\n",
        "        self.up3_2 = Upsample(int(dim*2**2))\n",
        "        self.reduce_chan_level2 = nn.Conv2d(int(dim*2**2), int(dim*2**1), kernel_size=1, bias=bias)\n",
        "        self.decoder_level2 = nn.Sequential(\n",
        "            *[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
        "                             bias=bias) for i in range(num_blocks[1])])\n",
        "\n",
        "        self.up2_1 = Upsample(int(dim*2**1))\n",
        "        self.decoder_level1 = nn.Sequential(\n",
        "            *[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor,\n",
        "                             bias=bias) for i in range(num_blocks[0])])\n",
        "\n",
        "        self.refinement = nn.Sequential(\n",
        "            *[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor,\n",
        "                             bias=bias) for i in range(num_refinement_blocks)])\n",
        "\n",
        "        self.output = nn.Conv2d(int(dim*2**1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
        "\n",
        "    def forward(self, inp_img):\n",
        "        inp_enc_level1 = self.patch_embed(inp_img)\n",
        "        out_enc_level1 = self.encoder_level1(inp_enc_level1)\n",
        "\n",
        "        inp_enc_level2 = self.down1_2(out_enc_level1)\n",
        "        out_enc_level2 = self.encoder_level2(inp_enc_level2)\n",
        "\n",
        "        inp_enc_level3 = self.down2_3(out_enc_level2)\n",
        "        out_enc_level3 = self.encoder_level3(inp_enc_level3)\n",
        "\n",
        "        inp_enc_level4 = self.down3_4(out_enc_level3)\n",
        "        latent = self.latent(inp_enc_level4)\n",
        "\n",
        "        inp_dec_level3 = self.up4_3(latent)\n",
        "        inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)\n",
        "        inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)\n",
        "        out_dec_level3 = self.decoder_level3(inp_dec_level3)\n",
        "\n",
        "        inp_dec_level2 = self.up3_2(out_dec_level3)\n",
        "        inp_dec_level2 = torch.cat([inp_dec_level2, out_enc_level2], 1)\n",
        "        inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)\n",
        "        out_dec_level2 = self.decoder_level2(inp_dec_level2)\n",
        "\n",
        "        inp_dec_level1 = self.up2_1(out_dec_level2)\n",
        "        inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)\n",
        "        out_dec_level1 = self.decoder_level1(inp_dec_level1)\n",
        "\n",
        "        out_dec_level1 = self.refinement(out_dec_level1)\n",
        "\n",
        "        out_dec_level1 = self.output(out_dec_level1) + inp_img\n",
        "\n",
        "        return out_dec_level1\n"
      ],
      "metadata": {
        "id": "n-xwCV8gxlLD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== Dataset ====================\n",
        "class GoPRODataset(Dataset):\n",
        "    def __init__(self, blur_dir, sharp_dir, transform=None):\n",
        "        self.blur_dir = Path(blur_dir)\n",
        "        self.sharp_dir = Path(sharp_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "        blur_images = list(self.blur_dir.glob('*.png')) + list(self.blur_dir.glob('*.jpg'))\n",
        "        sharp_images = list(self.sharp_dir.glob('*.png')) + list(self.sharp_dir.glob('*.jpg'))\n",
        "\n",
        "        print(f\"Found {len(blur_images)} blur images\")\n",
        "        print(f\"Found {len(sharp_images)} sharp images\")\n",
        "\n",
        "        blur_dict = {img.name: img for img in blur_images}\n",
        "        sharp_dict = {img.name: img for img in sharp_images}\n",
        "\n",
        "        common_names = sorted(set(blur_dict.keys()) & set(sharp_dict.keys()))\n",
        "\n",
        "        if len(common_names) == 0:\n",
        "            raise ValueError(\"No matching image pairs found!\")\n",
        "\n",
        "        self.blur_images = [blur_dict[name] for name in common_names]\n",
        "        self.sharp_images = [sharp_dict[name] for name in common_names]\n",
        "\n",
        "        print(f\"Using {len(self.blur_images)} matched image pairs\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.blur_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        blur_img = Image.open(self.blur_images[idx]).convert('RGB')\n",
        "        sharp_img = Image.open(self.sharp_images[idx]).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            blur_img = self.transform(blur_img)\n",
        "            sharp_img = self.transform(sharp_img)\n",
        "\n",
        "        return blur_img, sharp_img\n",
        "\n"
      ],
      "metadata": {
        "id": "Y86Yn7-8yA9-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==================== Loss Functions ====================\n",
        "class CharbonnierLoss(nn.Module):\n",
        "    \"\"\"Charbonnier Loss (L1 smooth loss)\"\"\"\n",
        "    def __init__(self, eps=1e-3):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        diff = pred - target\n",
        "        loss = torch.mean(torch.sqrt(diff * diff + self.eps))\n",
        "        return loss\n",
        "\n",
        "\n",
        "class EdgeLoss(nn.Module):\n",
        "    \"\"\"Edge-aware loss\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        k = torch.Tensor([[.05, .25, .4, .25, .05]])\n",
        "        self.kernel = torch.matmul(k.t(), k).unsqueeze(0).repeat(3, 1, 1, 1)\n",
        "        if torch.cuda.is_available():\n",
        "            self.kernel = self.kernel.cuda()\n",
        "        self.loss = CharbonnierLoss()\n",
        "\n",
        "    def conv_gauss(self, img):\n",
        "        n_channels, _, kw, kh = self.kernel.shape\n",
        "        img = nn.functional.pad(img, (kw//2, kh//2, kw//2, kh//2), mode='replicate')\n",
        "        return nn.functional.conv2d(img, self.kernel, groups=n_channels)\n",
        "\n",
        "    def laplacian_kernel(self, current):\n",
        "        filtered = self.conv_gauss(current)\n",
        "        down = filtered[:, :, ::2, ::2]\n",
        "        new_filter = torch.zeros_like(filtered)\n",
        "        new_filter[:, :, ::2, ::2] = down * 4\n",
        "        filtered = self.conv_gauss(new_filter)\n",
        "        diff = current - filtered\n",
        "        return diff\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        loss = self.loss(self.laplacian_kernel(pred), self.laplacian_kernel(target))\n",
        "        return loss"
      ],
      "metadata": {
        "id": "E00JdbLwyF0y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== Training ====================\n",
        "class RestormerTrainer:\n",
        "    def __init__(self, train_loader, val_loader, device='cuda'):\n",
        "        self.device = device\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "\n",
        "        # Model\n",
        "        self.model = Restormer(\n",
        "            inp_channels=3,\n",
        "            out_channels=3,\n",
        "            dim=48,\n",
        "            num_blocks=[4, 6, 6, 8],\n",
        "            num_refinement_blocks=4,\n",
        "            heads=[1, 2, 4, 8],\n",
        "            ffn_expansion_factor=2.66,\n",
        "            bias=False\n",
        "        ).to(device)\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.AdamW(self.model.parameters(), lr=3e-4, betas=(0.9, 0.999),\n",
        "                                     weight_decay=1e-4)\n",
        "\n",
        "        # Scheduler\n",
        "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=50, eta_min=1e-6)\n",
        "\n",
        "        # Loss functions\n",
        "        self.char_loss = CharbonnierLoss()\n",
        "        self.edge_loss = EdgeLoss()\n",
        "\n",
        "        # Tracking\n",
        "        self.history = {'train_loss': [], 'val_loss': [], 'val_psnr': []}\n",
        "\n",
        "    def calculate_psnr(self, pred, target):\n",
        "        \"\"\"Calculate PSNR\"\"\"\n",
        "        mse = torch.mean((pred - target) ** 2)\n",
        "        if mse == 0:\n",
        "            return 100\n",
        "        psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
        "        return psnr.item()\n",
        "\n",
        "    def train_epoch(self, epoch):\n",
        "        self.model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}')\n",
        "        for blur_imgs, sharp_imgs in pbar:\n",
        "            blur_imgs = blur_imgs.to(self.device)\n",
        "            sharp_imgs = sharp_imgs.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            restored = self.model(blur_imgs)\n",
        "\n",
        "            # Combined loss\n",
        "            loss_char = self.char_loss(restored, sharp_imgs)\n",
        "            loss_edge = self.edge_loss(restored, sharp_imgs)\n",
        "            loss = loss_char + (0.05 * loss_edge)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.01)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        return epoch_loss / len(self.train_loader)\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        val_loss = 0\n",
        "        psnrs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for blur_imgs, sharp_imgs in self.val_loader:\n",
        "                blur_imgs = blur_imgs.to(self.device)\n",
        "                sharp_imgs = sharp_imgs.to(self.device)\n",
        "\n",
        "                restored = self.model(blur_imgs)\n",
        "\n",
        "                loss = self.char_loss(restored, sharp_imgs)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                psnr = self.calculate_psnr(restored, sharp_imgs)\n",
        "                psnrs.append(psnr)\n",
        "\n",
        "        return val_loss / len(self.val_loader), np.mean(psnrs)\n",
        "\n",
        "    def train(self, num_epochs=100, save_dir='checkpoints'):\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        best_psnr = 0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss = self.train_epoch(epoch)\n",
        "            val_loss, val_psnr = self.validate()\n",
        "\n",
        "            self.scheduler.step()\n",
        "\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "            self.history['val_loss'].append(val_loss)\n",
        "            self.history['val_psnr'].append(val_psnr)\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "            print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val PSNR: {val_psnr:.2f} dB')\n",
        "            print(f'LR: {self.optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "            print('-' * 50)\n",
        "\n",
        "            # Save best model\n",
        "            if val_psnr > best_psnr:\n",
        "                best_psnr = val_psnr\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'best_psnr': best_psnr,\n",
        "                    'scheduler_state_dict': self.scheduler.state_dict()\n",
        "                }, f'{save_dir}/best_model.pth')\n",
        "                print(f'âœ… Saved best model with PSNR: {best_psnr:.2f} dB')\n",
        "\n",
        "            # Save checkpoint every 10 epochs\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'psnr': val_psnr\n",
        "                }, f'{save_dir}/checkpoint_epoch_{epoch+1}.pth')\n",
        "\n",
        "        self.plot_history(save_dir)\n",
        "\n",
        "    def plot_history(self, save_dir):\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "        axes[0].plot(self.history['train_loss'], label='Train')\n",
        "        axes[0].plot(self.history['val_loss'], label='Val')\n",
        "        axes[0].set_title('Loss')\n",
        "        axes[0].set_xlabel('Epoch')\n",
        "        axes[0].set_ylabel('Loss')\n",
        "        axes[0].legend()\n",
        "\n",
        "        axes[1].plot(self.history['val_psnr'])\n",
        "        axes[1].set_title('Validation PSNR')\n",
        "        axes[1].set_xlabel('Epoch')\n",
        "        axes[1].set_ylabel('PSNR (dB)')\n",
        "\n",
        "        axes[2].plot([self.optimizer.param_groups[0]['lr'] for _ in range(len(self.history['train_loss']))])\n",
        "        axes[2].set_title('Learning Rate')\n",
        "        axes[2].set_xlabel('Epoch')\n",
        "        axes[2].set_ylabel('LR')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{save_dir}/training_history.png')\n",
        "        plt.close()\n"
      ],
      "metadata": {
        "id": "vkzS8IfuyKpG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== Main ====================\n",
        "def main():\n",
        "    # Mount Google Drive (for Colab)\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted successfully!\")\n",
        "    except:\n",
        "        print(\"Not running in Colab or Drive already mounted\")\n",
        "\n",
        "    # Configuration\n",
        "    DATASET_PATH = \"/content/drive/MyDrive/CV/gopro_deblur\"\n",
        "    BLUR_DIR = f'{DATASET_PATH}/blur/images'\n",
        "    SHARP_DIR = f'{DATASET_PATH}/sharp/images'\n",
        "    BATCH_SIZE = 4  # Restormer is memory intensive\n",
        "    NUM_EPOCHS = 5\n",
        "    IMG_SIZE = 256\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "        print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
        "\n",
        "    # Transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # Dataset\n",
        "    dataset = GoPRODataset(BLUR_DIR, SHARP_DIR, transform=transform)\n",
        "\n",
        "    # Split dataset\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size]\n",
        "    )\n",
        "\n",
        "    print(f'Train size: {len(train_dataset)}, Val size: {len(val_dataset)}')\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                            shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
        "                          shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    # Save checkpoints to Drive\n",
        "    CHECKPOINT_DIR = '/content/drive/MyDrive/CV/restormer_checkpoints'\n",
        "\n",
        "    # Train\n",
        "    trainer = RestormerTrainer(train_loader, val_loader, device=device)\n",
        "\n",
        "    # Print model size\n",
        "    total_params = sum(p.numel() for p in trainer.model.parameters())\n",
        "    print(f'Total parameters: {total_params / 1e6:.2f}M')\n",
        "\n",
        "    trainer.train(num_epochs=NUM_EPOCHS, save_dir=CHECKPOINT_DIR)\n",
        "\n",
        "    print('Training completed!')\n",
        "    print(f'Checkpoints saved to: {CHECKPOINT_DIR}')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "SxSa0w0OyMop",
        "outputId": "47acb8e6-2891-49c2-bbb3-c2740fa53c86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3125178535.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3125178535.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mIMG_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Using device: {device}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zfjM7tftzUAS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}